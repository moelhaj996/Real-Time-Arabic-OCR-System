# Training Configuration

# Data
data:
  train_data_path: "data/processed/train"
  val_data_path: "data/processed/val"
  test_data_path: "data/processed/test"
  synthetic_data_path: "data/augmented"

  # Data split ratios
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

  # Dataset size
  synthetic_samples: 100000
  use_real_data: true
  real_data_weight: 2.0  # Weight real data 2x vs synthetic

# Training hyperparameters
training:
  # Phases
  num_epochs: 80

  phase1:  # Synthetic data pretraining
    epochs: 50
    datasets: ["synthetic"]

  phase2:  # Real data fine-tuning
    epochs: 30
    datasets: ["synthetic", "real"]
    freeze_encoder: false

  # Batch size
  batch_size: 32
  effective_batch_size: 128  # Gradient accumulation
  gradient_accumulation_steps: 4

  # Optimizer
  optimizer:
    type: "adamw"
    learning_rate: 5.0e-5
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.01

  # Learning rate schedule
  scheduler:
    type: "cosine"
    warmup_steps: 10000
    warmup_ratio: 0.1
    min_lr: 1.0e-7

  # Gradient clipping
  max_grad_norm: 1.0

  # Mixed precision
  use_amp: true
  amp_dtype: "float16"  # or bfloat16

# Loss function
loss:
  type: "hybrid"  # hybrid, ctc, cross_entropy

  # Hybrid loss weights
  ctc_weight: 0.3
  ce_weight: 0.7

  # Label smoothing
  label_smoothing: 0.1

  # Auxiliary losses
  use_attention_loss: false
  attention_loss_weight: 0.1

# Regularization
regularization:
  dropout: 0.1
  attention_dropout: 0.1
  drop_path: 0.1
  layer_dropout: 0.0

# Evaluation
evaluation:
  eval_every_n_steps: 1000
  save_every_n_steps: 5000
  eval_batch_size: 64

  # Metrics
  metrics:
    - "cer"  # Character Error Rate
    - "wer"  # Word Error Rate
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"

  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    metric: "cer"
    mode: "min"

# Checkpointing
checkpointing:
  save_dir: "models/checkpoints"
  save_top_k: 3
  monitor: "val_cer"
  mode: "min"
  save_last: true

# Logging
logging:
  log_every_n_steps: 100
  log_dir: "logs"

  # Weights & Biases
  use_wandb: false
  wandb_project: "arabic-ocr"
  wandb_entity: null

  # TensorBoard
  use_tensorboard: true

# Reproducibility
seed: 42
deterministic: false  # Set to true for full reproducibility (slower)

# Hardware
hardware:
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

  # Multi-GPU
  gpus: 1  # Number of GPUs (-1 for all available)
  strategy: "ddp"  # ddp, ddp_spawn, deepspeed
  precision: 16  # 16, 32, bf16

# Debugging
debug:
  enabled: false
  overfit_batches: 0
  fast_dev_run: false
  limit_train_batches: 1.0
  limit_val_batches: 1.0
