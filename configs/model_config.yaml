# Model Architecture Configuration

model:
  type: "vit_transformer_ocr"  # vit_transformer_ocr, efficientnet_ocr, mobilenet_ocr

  # Vision Transformer Encoder
  encoder:
    name: "vit_base_patch16_384"
    pretrained: true
    img_size: 384
    patch_size: 16
    embed_dim: 768
    depth: 12
    num_heads: 12
    mlp_ratio: 4.0
    dropout: 0.1
    attention_dropout: 0.1

  # Transformer Decoder
  decoder:
    vocab_size: 3500  # Arabic chars + special tokens + ligatures
    max_seq_length: 256
    embed_dim: 768
    num_layers: 6
    num_heads: 8
    ffn_dim: 2048
    dropout: 0.1
    attention_dropout: 0.1
    activation: "gelu"

  # Alternative lightweight models
  lite:
    encoder_name: "mobilenetv3_large_100"
    decoder_layers: 4
    embed_dim: 512

  edge:
    encoder_name: "efficientnet_b0"
    decoder_layers: 3
    embed_dim: 384

# Vocabulary Configuration
vocabulary:
  # Arabic Unicode ranges
  arabic_letters: true  # U+0600 to U+06FF
  arabic_supplement: true  # U+0750 to U+077F
  extended_arabic: true  # U+08A0 to U+08FF

  # Special tokens
  special_tokens:
    pad_token: "<PAD>"
    sos_token: "<SOS>"
    eos_token: "<EOS>"
    unk_token: "<UNK>"

  # Include diacritics
  diacritics: true

  # Common ligatures
  ligatures: true

# Input Processing
preprocessing:
  target_size: [384, 384]
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

  # Augmentation (training only)
  augmentation:
    rotation_range: 15
    width_shift: 0.1
    height_shift: 0.1
    zoom_range: 0.15
    horizontal_flip: false  # Not suitable for text
    gaussian_noise: 0.01
    brightness_range: [0.7, 1.3]
    contrast_range: [0.8, 1.2]

# Output Configuration
postprocessing:
  beam_width: 5
  length_penalty: 0.6
  temperature: 1.0
  confidence_threshold: 0.5
  use_language_model: true
  rtl_support: true  # Right-to-left text support
